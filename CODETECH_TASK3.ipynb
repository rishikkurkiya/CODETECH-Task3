{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## PREDICTIVE MODELING WITH CLASSIFICATION"
      ],
      "metadata": {
        "id": "h-Nq7ncuJgVQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing libraries"
      ],
      "metadata": {
        "id": "inr8Bu3IJki1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n"
      ],
      "metadata": {
        "id": "NidcQJM3Jo_B"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading and Preprocessing Data"
      ],
      "metadata": {
        "id": "hpfsSnYYJ3CJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Iris dataset\n",
        "from sklearn.datasets import load_iris\n",
        "data = load_iris()\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "# Preprocess the data (Standardization)\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n"
      ],
      "metadata": {
        "id": "hmsAk1zTJ6wE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Initializing and Training Models"
      ],
      "metadata": {
        "id": "EGoDLeTaJ8h2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize classifiers\n",
        "classifiers = {\n",
        "    'Logistic Regression': LogisticRegression(),\n",
        "    'Decision Tree': DecisionTreeClassifier(),\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'Support Vector Machine': SVC()\n",
        "}\n",
        "\n",
        "# Define scoring metrics\n",
        "scoring = {\n",
        "    'accuracy': make_scorer(accuracy_score),\n",
        "    'precision': make_scorer(precision_score, average='weighted'),\n",
        "    'recall': make_scorer(recall_score, average='weighted'),\n",
        "    'f1_score': make_scorer(f1_score, average='weighted')\n",
        "}\n",
        "\n",
        "# Train and evaluate each classifier using cross-validation\n",
        "results = {}\n",
        "for name, clf in classifiers.items():\n",
        "    scores = {}\n",
        "    for metric_name, metric in scoring.items():\n",
        "        score = cross_val_score(clf, X_scaled, y, cv=5, scoring=metric)\n",
        "        scores[metric_name] = np.mean(score)\n",
        "    results[name] = scores\n"
      ],
      "metadata": {
        "id": "M2kRmLDNKFrS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Printing and Comparing Results"
      ],
      "metadata": {
        "id": "zu-S9Y8uKGnj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdVlD_ZMJMoa",
        "outputId": "f9f9acb5-55bb-424a-8bc0-0278c2cfeb40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classifier: Logistic Regression\n",
            "Accuracy: 0.9600\n",
            "Precision: 0.9633\n",
            "Recall: 0.9600\n",
            "F1 Score: 0.9598\n",
            "------------------------------\n",
            "Classifier: Decision Tree\n",
            "Accuracy: 0.9667\n",
            "Precision: 0.9550\n",
            "Recall: 0.9533\n",
            "F1 Score: 0.9599\n",
            "------------------------------\n",
            "Classifier: Random Forest\n",
            "Accuracy: 0.9667\n",
            "Precision: 0.9592\n",
            "Recall: 0.9667\n",
            "F1 Score: 0.9531\n",
            "------------------------------\n",
            "Classifier: Support Vector Machine\n",
            "Accuracy: 0.9667\n",
            "Precision: 0.9685\n",
            "Recall: 0.9667\n",
            "F1 Score: 0.9666\n",
            "------------------------------\n",
            "The best model is: Support Vector Machine with F1 Score: 0.9666\n"
          ]
        }
      ],
      "source": [
        "# Print the results\n",
        "for name, metrics in results.items():\n",
        "    print(f\"Classifier: {name}\")\n",
        "    print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
        "    print(f\"Precision: {metrics['precision']:.4f}\")\n",
        "    print(f\"Recall: {metrics['recall']:.4f}\")\n",
        "    print(f\"F1 Score: {metrics['f1_score']:.4f}\")\n",
        "    print('-'*30)\n",
        "\n",
        "# Compare the performances\n",
        "best_model = max(results, key=lambda x: results[x]['f1_score'])\n",
        "print(f\"The best model is: {best_model} with F1 Score: {results[best_model]['f1_score']:.4f}\")\n"
      ]
    }
  ]
}